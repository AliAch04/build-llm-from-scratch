{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyjs33AiAvlz2Sm1ckxPYY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliAch04/build-llm-from-scratch/blob/main/LLMScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Short Story as text sample Into python"
      ],
      "metadata": {
        "id": "_kncVckNQoJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1 : Create Tokens**"
      ],
      "metadata": {
        "id": "3DDRpKV5Q2QF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"External/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print('Number of characters : ', len(text))\n",
        "# print first 100 characters\n",
        "print(text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSKrSmDgRV94",
        "outputId": "0a473e68-595f-4155-c183-40aee943234a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of characters :  20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tokenize the whole characters (20479) into individual words and special characters Then turn into embeddings"
      ],
      "metadata": {
        "id": "sd8v3ca7S5tO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Split text into list of tokens based on white text or special characters ..."
      ],
      "metadata": {
        "id": "ISuFpbq-T3ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re # Regular expression\n",
        "\n",
        "text_test = \"Hello world. this, is a test.\"\n",
        "result = re.split(r'(\\S)', text_test)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l31a3V0vT8J6",
        "outputId": "e063896c-3a3b-42bb-a49d-55d40b7ce5ef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'H', '', 'e', '', 'l', '', 'l', '', 'o', ' ', 'w', '', 'o', '', 'r', '', 'l', '', 'd', '', '.', ' ', 't', '', 'h', '', 'i', '', 's', '', ',', ' ', 'i', '', 's', ' ', 'a', ' ', 't', '', 'e', '', 's', '', 't', '', '.', '']\n"
          ]
        }
      ]
    }
  ]
}