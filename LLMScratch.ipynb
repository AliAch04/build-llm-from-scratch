{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1yKQ8OmmRweTluexcIKZpmwlhjJSR27v1",
      "authorship_tag": "ABX9TyOYqHxBCeaqNZSTmEAW8SPm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliAch04/build-llm-from-scratch/blob/main/LLMScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Short Story as text sample Into python"
      ],
      "metadata": {
        "id": "_kncVckNQoJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1 : Create Tokens**"
      ],
      "metadata": {
        "id": "3DDRpKV5Q2QF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path1 = \"/content/sample_data/the-verdict.txt\"\n",
        "path2 = \"/content/drive/MyDrive/llm/the-verdict.txt\"\n",
        "\n",
        "# Check if the first path exists\n",
        "if os.path.exists(path1):\n",
        "    file_path = path1\n",
        "    print(f\"Using primary path: {file_path}\")\n",
        "else:\n",
        "    file_path = path2\n",
        "    print(f\"Primary path not found. Using secondary path: {file_path}\")\n",
        "\n",
        "try:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_text = f.read()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file at {file_path} was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the file: {e}\")\n",
        "\n",
        "print('Number of characters : ', len(raw_text))\n",
        "# print first 100 characters\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSKrSmDgRV94",
        "outputId": "91571b18-7ffd-4dba-9f81-45800819ad6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using primary path: /content/sample_data/the-verdict.txt\n",
            "Number of characters :  20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tokenize the whole characters (20479) into individual words and special characters Then turn into embeddings"
      ],
      "metadata": {
        "id": "sd8v3ca7S5tO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Split text into list of tokens based on white text or special characters ..."
      ],
      "metadata": {
        "id": "ISuFpbq-T3ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re # Regular expression\n",
        "\n",
        "text_test = \"Hello, world. this, is a test!\"\n",
        "result = re.split(r'([,.!]|\\s)', text_test)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l31a3V0vT8J6",
        "outputId": "badcb0e4-1ebc-419a-87db-7ea153f23839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'this', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '!', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Remove redundants characters safely"
      ],
      "metadata": {
        "id": "BrDji5hyWqFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#result = [item for item in result if item not in ['', ' ']]\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3xPAG8BW4L5",
        "outputId": "8fe0b290-2ceb-457a-db4a-92ca4b7e5996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'this', ',', 'is', 'a', 'test', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In our context we are removing the white-spaces because our text structure doesnt need it (working on simple sample of text)"
      ],
      "metadata": {
        "id": "5cLep7J5XrZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Full process"
      ],
      "metadata": {
        "id": "a6cGs5t7Yp6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_test = 'I HAD always! thought Jack Gisburn rather? a cheap genius--though a good fellow enough--so it was no! '\n",
        "# Tokenization sheme\n",
        "result = re.split(r'([.,:;!_?\"]|--|\\s)', text_test)\n",
        "result = [i for i in result if i.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoXBXOuzYtoZ",
        "outputId": "82ce7e87-a8e9-4282-a3d1-91c55b9a627e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', '!', 'thought', 'Jack', 'Gisburn', 'rather', '?', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Apply the tokenizer on the Story"
      ],
      "metadata": {
        "id": "kOJyYJj0aT2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;_!?\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpjdEC9EaTi5",
        "outputId": "e546168c-ad90-4c00-bdd6-053dc149c78f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total number of tokens : ', len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4ue14ktbMd8",
        "outputId": "ae4ef1df-cd66-49c6-a59b-547eb9c688f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens :  4690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2 : Create Tokens IDs**"
      ],
      "metadata": {
        "id": "RlbMVcB4cNtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create list of tokens and sort them alphabetically to determine the Vocabilary size"
      ],
      "metadata": {
        "id": "ISNvfRHRcXJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "print(all_words[:15])\n",
        "print(len(all_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H0pGtxfcw0G",
        "outputId": "2de288c3-d866-4e6b-96c7-c3db83759035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';', '?', 'A', 'Ah', 'Among', 'And']\n",
            "1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create the Vocabulary itself"
      ],
      "metadata": {
        "id": "wwPXO8AUcwT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token : i for i, token in enumerate(all_words)}\n"
      ],
      "metadata": {
        "id": "uH5sOlmddWHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, v in enumerate(vocab.items()):\n",
        "  if i > 25:\n",
        "    break\n",
        "  print(v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZdJ5ppO-zPp",
        "outputId": "6ed721d8-6d17-43e2-efd9-06ce04e14352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Emplement Tokenizer class"
      ],
      "metadata": {
        "id": "rQRxRcGTBpBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer:\n",
        "  def __init__(self):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {id:t for t, id in vocab.items()}\n",
        "\n",
        "  def encoder(self, txt):\n",
        "    processed = re.split(r'([.,;?()!_:\\'\"]|--|\\s)', txt)\n",
        "\n",
        "    processed = [item.strip() for item in processed if item.strip()]\n",
        "\n",
        "    ids = [self.str_to_int[t] for t in processed]\n",
        "    return ids\n",
        "\n",
        "  def decoder(self, ids):\n",
        "    txt = \" \".join([self.int_to_str[id] for id in ids])\n",
        "    # Prevent the whitespace before the punctuation marks\n",
        "    txt = re.sub(r'\\s+([,.\"\\'?!()])', r'\\1', txt)\n",
        "    txt = re.sub(r'([\"\\'])\\s+(\\w+)', r'\\1\\2', txt)\n",
        "    return txt\n",
        "\n",
        "token = SimpleTokenizer()\n",
        "print(token.encoder('Ah! At \"Among\" !'))\n",
        "print(token.decoder(token.encoder('Ah! \\'At\\':\"Among \" !')))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNhwgzmHZnoW",
        "outputId": "1437efde-8f78-42c8-e8bc-370b621d6edb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12, 0, 18, 1, 13, 1, 0]\n",
            "Ah!'At' :\"Among\"!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To address words not in the vocabulary, the vocabulary needs to be extended."
      ],
      "metadata": {
        "id": "vgq8nmW2kbBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding Special Context Tokens**"
      ],
      "metadata": {
        "id": "whP6AbgxlfkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Modify the Tokenizer to handle unkonwn words (<|unk|> <|endoftext|>)"
      ],
      "metadata": {
        "id": "-cUA1q1XLC1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend(['<|unk|>', '<|endoftext|>'])\n",
        "\n",
        "vocab = {t:i for i, t in enumerate(all_tokens)}\n",
        "print(len(vocab.items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfif9CyiOBfp",
        "outputId": "d00811a0-4cff-43b2-cb20-365b50f10619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item in list(vocab.items())[-5:]:\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRQs7s3gOBdn",
        "outputId": "26697daf-f819-4435-d7d8-a260d9bdb0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|unk|>', 1130)\n",
            "('<|endoftext|>', 1131)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Implementing SimpleTokenizerV2"
      ],
      "metadata": {
        "id": "Db8y53hBQgct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:t for t,i in vocab.items()}\n",
        "\n",
        "  def encoder(self, txt):\n",
        "    # Split Text into tokens\n",
        "    preprocessed = re.split(r'([.,;?()!_:\\'\"]|--|\\s)', txt)\n",
        "    # Clear the Tokens from whitespaces\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    # Identifiy the unknown words\n",
        "    preprocessed = [\n",
        "        item if item in self.str_to_int else '<|unk|>'\n",
        "        for item in preprocessed\n",
        "    ]\n",
        "    # Map the tokens with thier ids\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "\n",
        "    return ids\n",
        "\n",
        "  def decoder(self, ids):\n",
        "    txt = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    txt = re.sub(r'\\s+([,.;:!?()\"\\'])', r'\\1', txt)\n",
        "\n",
        "    return txt"
      ],
      "metadata": {
        "id": "R2-_ctXoQk1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "text1 = 'Salam! I love tea so much. What about you?'\n",
        "text2 = 'Yes! i love tea and coffee.'\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBxgq9zfY32c",
        "outputId": "3fa6691b-316a-43f9-81ef-472c3f34c1f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Salam! I love tea so much. What about you? <|endoftext|> Yes! i love tea and coffee.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.encoder(text))\n",
        "print(tokenizer.decoder(tokenizer.encoder(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7RDUq6NaThQ",
        "outputId": "a2e69141-1a92-4f6d-b541-b60f8f9070a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1130, 0, 53, 1130, 975, 908, 691, 7, 109, 118, 1126, 10, 1131, 112, 0, 1130, 1130, 975, 157, 1130, 7]\n",
            "<|unk|>! I <|unk|> tea so much. What about you? <|endoftext|> Yes! <|unk|> <|unk|> tea and <|unk|>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- More Special tokens : [BOS] [EOS] [PAD]"
      ],
      "metadata": {
        "id": "8jbF_pBgdI4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For GPT models they doesnt use <|unk|> tokens. Instead they uses a tokenizer called **Byte Pair Encoding** which beaks words into subword units."
      ],
      "metadata": {
        "id": "sk08vKOBaQDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Byte Pair Encoding**"
      ],
      "metadata": {
        "id": "GoH-SPckerCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Use pyhton open-source library '**tiktoken**' 'https://github.com/openai/tiktoken'"
      ],
      "metadata": {
        "id": "YovHjfoKgVtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip3 install tiktoken"
      ],
      "metadata": {
        "id": "QGVgfQXEhcp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the tiktoken package and check its version\n",
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print('Tiktoken version : ', importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzBNqXi_iYTA",
        "outputId": "90fb7e5c-4964-45e4-e236-fefe87a9a7b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiktoken version :  0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate BPE tokenizer"
      ],
      "metadata": {
        "id": "kUaROrCDjDDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "GnGPGTMwjKDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\"Hello! do you like tea? <|endoftext|> The sun rose quickly this morning.\"\n",
        "\"what do you thinkofme me?\")\n",
        "\n",
        "ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(ids)\n",
        "print(tokenizer.decode([1326, 502]))  # Should show \"me\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9b5DOhBjxq8",
        "outputId": "c68d48e3-5964-4efe-ca93-9896b6f03eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 0, 466, 345, 588, 8887, 30, 220, 50256, 383, 4252, 8278, 2952, 428, 3329, 13, 10919, 466, 345, 892, 1659, 1326, 502, 30]\n",
            "me me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.decode(ids)\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TbaSFa2oXzq",
        "outputId": "b3eb0cae-fe42-498c-b123-dd9a0032f5f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! do you like tea? <|endoftext|> The sun rose quickly this morning.what do you thinkofme me?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- More example to illustrate BPE tokenizer deals with unknown tokens"
      ],
      "metadata": {
        "id": "7Mmshtkrrs_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids =  tokenizer.encode('Awki ingze')\n",
        "print(ids)\n",
        "\n",
        "txt = tokenizer.decode(ids)\n",
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAbKgI2Wr7aQ",
        "outputId": "0d4ce310-0545-4c5c-ae46-4a4415082410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[23155, 4106, 5347, 2736]\n",
            "Awki ingze\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Input-target pairs**"
      ],
      "metadata": {
        "id": "i15Im0ayeBsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Implement a data loader that fetchs the input-target pairs using a sliding window approach*"
      ],
      "metadata": {
        "id": "XSnslwQXoyzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tokenize the whole dataset (The verdict) using BPE tokenizer"
      ],
      "metadata": {
        "id": "42riiMChp6P1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))\n",
        "print(len(raw_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAzJkAaCqISq",
        "outputId": "07fc9233-deaf-4b78-b34d-412de3a54821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n",
            "20479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Define the context size (how many tokens in the input)\n",
        "\n"
      ],
      "metadata": {
        "id": "RnXKz4Dptcxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_example = enc_text[50:] # gives intresting result\n",
        "context_size = 4 # so the model will look at 4 sequences of words to predict the next word in the sequence!\n",
        "\n",
        "x = enc_example[:context_size]\n",
        "y = enc_example[1:context_size+1]\n",
        "# the target is just the input shifted by 1 (so we can capture the next-word that we need to predict)\n",
        "\n",
        "print(f'{x}')\n",
        "print(f'     {y}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtQKFrEEqH76",
        "outputId": "c5136d04-6e3f-4797-b3d6-f4c1b0e7cf91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290, 4920, 2241, 287]\n",
            "     [4920, 2241, 287, 257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create the next-word prediction tasks"
      ],
      "metadata": {
        "id": "FKscwpSPvyrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "  context = enc_example[:i]\n",
        "  next_word = enc_example[i] # supposed to predict\n",
        "  print(f'{context}->{next_word}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y96jmBfcwT53",
        "outputId": "02e4ee7f-1a4b-4ca4-b626-c8d6b733255d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290]->4920\n",
            "[290, 4920]->2241\n",
            "[290, 4920, 2241]->287\n",
            "[290, 4920, 2241, 287]->257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "  context = enc_example[:i]\n",
        "  next_word = enc_example[i]\n",
        "  print(tokenizer.decode(context),'->',tokenizer.decode([next_word]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4GvSq2Y11Js",
        "outputId": "0edd52ba-967e-4896-e155-ce88c0708a34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ->  established\n",
            " and established ->  himself\n",
            " and established himself ->  in\n",
            " and established himself in ->  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Turning the inputs and targets as PyThorch tensors (required for the income optimazation procedures)"
      ],
      "metadata": {
        "id": "PlP9i1Hu3mON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing a Data Loader**"
      ],
      "metadata": {
        "id": "Xbgd-xVi6DpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The methodology is:\n",
        "\n",
        "\n",
        "1.   Firstly, tokenize the text\n",
        "2.   Using the sliding window to make two overlapping sequences of the text (lenght of each row is the context size)\n",
        "3.   List item\n",
        "4.   List item\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tw52QMqK9QJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "\n",
        "class GPTDataset(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_lenght, stride):\n",
        "     self.input_ids = []\n",
        "     self.target_ids = []\n",
        "\n",
        "     token_ids = tokenizer.encode(txt, allowed_special={'<|endoftext|>'})\n",
        "\n",
        "     for i in range(0, len(token_ids) - max_lenght, stride):\n",
        "        input_chunk = token_ids[i:i + max_lenght]\n",
        "        target_chunk = token_ids[i+1:i + max_lenght+1]\n",
        "        self.input_ids.append(torch.tensor(input_chunk))\n",
        "        self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "     return len(self.input_ids)\n",
        "\n",
        "  # accessing item by [index]\n",
        "  def __getitem__(self, idx):\n",
        "     return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "SGg4avQf-Cae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5mdaQxTb-COO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}